{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1462296,
          "sourceType": "datasetVersion",
          "datasetId": 857191
        },
        {
          "sourceId": 2907952,
          "sourceType": "datasetVersion",
          "datasetId": 1730795
        }
      ],
      "dockerImageVersionId": 30235,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/g00dAA/Image-Captioning-on-coco-dataset/blob/main/image_captioning_on_coco_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from math import sqrt\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "import collections\n",
        "import random\n",
        "import requests\n",
        "import json\n",
        "import pickle"
      ],
      "metadata": {
        "id": "AldVDvOgcpbc",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:23:37.723557Z",
          "iopub.execute_input": "2024-05-20T12:23:37.723986Z",
          "iopub.status.idle": "2024-05-20T12:23:37.732821Z",
          "shell.execute_reply.started": "2024-05-20T12:23:37.723953Z",
          "shell.execute_reply": "2024-05-20T12:23:37.731205Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = '../input/coco-2017-dataset/coco2017'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-20T12:04:55.366868Z",
          "iopub.execute_input": "2024-05-20T12:04:55.367811Z",
          "iopub.status.idle": "2024-05-20T12:04:55.373719Z",
          "shell.execute_reply.started": "2024-05-20T12:04:55.367769Z",
          "shell.execute_reply": "2024-05-20T12:04:55.372458Z"
        },
        "trusted": true,
        "id": "Zb0vPKdu395K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Open the JSON file containing the annotations**"
      ],
      "metadata": {
        "id": "F0Y9E1MF395L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(f'{BASE_PATH}/annotations/captions_train2017.json', 'r') as f:\n",
        "    data = json.load(f)\n",
        "    data = data['annotations']\n",
        "\n",
        "# Initialize a list to store image-caption pairs\n",
        "img_cap_pairs = []\n",
        "\n",
        "for sample in data:\n",
        "    img_name = '%012d.jpg' % sample['image_id']              # Format the image filename using the image ID\n",
        "    img_cap_pairs.append([img_name, sample['caption']])      # Append the image name and caption as a pair to the list\n",
        "\n",
        "\n",
        "# Creating the DataFrame\n",
        "captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
        "captions['image'] = captions['image'].apply(\n",
        "    lambda x: f'{BASE_PATH}/train2017/{x}'\n",
        ")\n",
        "\n",
        "# Randomly select 70,000 samples from the DataFrame to create a subset\n",
        "captions = captions.sample(70000)\n",
        "captions = captions.reset_index(drop=True)\n",
        "captions.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-20T12:05:01.032137Z",
          "iopub.execute_input": "2024-05-20T12:05:01.032573Z",
          "iopub.status.idle": "2024-05-20T12:05:05.851593Z",
          "shell.execute_reply.started": "2024-05-20T12:05:01.032539Z",
          "shell.execute_reply": "2024-05-20T12:05:05.850317Z"
        },
        "trusted": true,
        "id": "XdXIHSdl395N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing**"
      ],
      "metadata": {
        "id": "c42b8HTQ395O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()                     # Convert to lowercase\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)     # Remove punctuation\n",
        "    text = re.sub('\\s+', ' ', text)         # Replace multiple spaces with a single space\n",
        "    text = text.strip()                     # Strip leading and trailing spaces\n",
        "    text = '[start] ' + text + ' [end]'     # Add start and end tokens to the text\n",
        "    return text"
      ],
      "metadata": {
        "id": "rWbe_xuhFaJp",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:10:15.377259Z",
          "iopub.execute_input": "2024-05-20T12:10:15.377707Z",
          "iopub.status.idle": "2024-05-20T12:10:15.385720Z",
          "shell.execute_reply.started": "2024-05-20T12:10:15.377672Z",
          "shell.execute_reply": "2024-05-20T12:10:15.384326Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Preprocessing\n",
        "captions['caption'] = captions['caption'].apply(preprocess)\n",
        "captions.head()"
      ],
      "metadata": {
        "id": "v_ouwWhKnEy5",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:10:19.998046Z",
          "iopub.execute_input": "2024-05-20T12:10:19.998533Z",
          "iopub.status.idle": "2024-05-20T12:10:20.857884Z",
          "shell.execute_reply.started": "2024-05-20T12:10:19.998492Z",
          "shell.execute_reply": "2024-05-20T12:10:20.856749Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting a random row\n",
        "random_row = captions.sample(1).iloc[0]\n",
        "\n",
        "# Printing the caption associated with the image\n",
        "print(random_row.caption)\n",
        "print()\n",
        "\n",
        "im = Image.open(random_row.image)\n",
        "im"
      ],
      "metadata": {
        "id": "6RBuExHWnGEt",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:15:07.614760Z",
          "iopub.execute_input": "2024-05-20T12:15:07.615275Z",
          "iopub.status.idle": "2024-05-20T12:15:07.821244Z",
          "shell.execute_reply.started": "2024-05-20T12:15:07.615206Z",
          "shell.execute_reply": "2024-05-20T12:15:07.820001Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up the Hyperparameters**"
      ],
      "metadata": {
        "id": "Ei4o32mw395R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 40\n",
        "VOCABULARY_SIZE = 10000    # Reduced vocabulary size\n",
        "BATCH_SIZE = 32            # Reduced batch size\n",
        "BUFFER_SIZE = 2000         # Increased buffer size\n",
        "EMBEDDING_DIM = 256        # Reduced embedding dimension\n",
        "UNITS = 256                # Reduced number of units\n",
        "EPOCHS = 20                # Reduced number of epochs\n"
      ],
      "metadata": {
        "id": "nSTivH_FSSf2",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:22:31.828547Z",
          "iopub.execute_input": "2024-05-20T12:22:31.829022Z",
          "iopub.status.idle": "2024-05-20T12:22:31.835997Z",
          "shell.execute_reply.started": "2024-05-20T12:22:31.828985Z",
          "shell.execute_reply": "2024-05-20T12:22:31.834440Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a TextVectorization layer for tokenizing text captions\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=VOCABULARY_SIZE,                     # Setting the maximum size of the vocabulary\n",
        "    standardize=None,                               # No standardization is applied\n",
        "    output_sequence_length=MAX_LENGTH               # Setting the length of output sequences\n",
        ")\n",
        "\n",
        "# Adapting the tokenizer to the captions dataset\n",
        "tokenizer.adapt(captions['caption'])"
      ],
      "metadata": {
        "id": "X8MGUNtBN2sz",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:22:39.814319Z",
          "iopub.execute_input": "2024-05-20T12:22:39.814760Z",
          "iopub.status.idle": "2024-05-20T12:22:41.899874Z",
          "shell.execute_reply.started": "2024-05-20T12:22:39.814726Z",
          "shell.execute_reply": "2024-05-20T12:22:41.898702Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocabulary_size()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-20T12:22:46.668523Z",
          "iopub.execute_input": "2024-05-20T12:22:46.669034Z",
          "iopub.status.idle": "2024-05-20T12:22:46.677462Z",
          "shell.execute_reply.started": "2024-05-20T12:22:46.668989Z",
          "shell.execute_reply": "2024-05-20T12:22:46.676073Z"
        },
        "trusted": true,
        "id": "tA1877OZ395T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the vocabulary of the tokenizer to a binary file\n",
        "pickle.dump(tokenizer.get_vocabulary(), open('vocab_coco.file', 'wb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-20T12:23:49.112258Z",
          "iopub.execute_input": "2024-05-20T12:23:49.112758Z",
          "iopub.status.idle": "2024-05-20T12:23:49.252164Z",
          "shell.execute_reply.started": "2024-05-20T12:23:49.112719Z",
          "shell.execute_reply": "2024-05-20T12:23:49.250761Z"
        },
        "trusted": true,
        "id": "Mtr8fHn-395U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a StringLookup layer to map words to indices\n",
        "word2idx = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",  # No mask token\n",
        "    vocabulary=tokenizer.get_vocabulary()  # Use the tokenizer's vocabulary\n",
        ")\n",
        "\n",
        "# Creating a StringLookup layer to map indices back to words\n",
        "idx2word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",  # No mask token\n",
        "    vocabulary=tokenizer.get_vocabulary(),  # Use the tokenizer's vocabulary\n",
        "    invert=True  # Invert the mapping to go from indices to words\n",
        ")"
      ],
      "metadata": {
        "id": "qvhg-6eKN3nz",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:27:43.488705Z",
          "iopub.execute_input": "2024-05-20T12:27:43.489187Z",
          "iopub.status.idle": "2024-05-20T12:27:43.825591Z",
          "shell.execute_reply.started": "2024-05-20T12:27:43.489151Z",
          "shell.execute_reply": "2024-05-20T12:27:43.824185Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dictionary to map each image to its corresponding captions\n",
        "img_to_cap_vector = collections.defaultdict(list)\n",
        "\n",
        "# Populate the dictionary with image-caption pairs\n",
        "for img, cap in zip(captions['image'], captions['caption']):\n",
        "    img_to_cap_vector[img].append(cap)\n",
        "\n",
        "# Get a list of all image keys (file paths)\n",
        "img_keys = list(img_to_cap_vector.keys())\n",
        "random.shuffle(img_keys)\n",
        "\n",
        "slice_index = int(len(img_keys)*0.8)\n",
        "# Split the keys into training and validation sets\n",
        "img_name_train_keys, img_name_val_keys = (img_keys[:slice_index],\n",
        "                                          img_keys[slice_index:])"
      ],
      "metadata": {
        "id": "Yrca2aN2N5WL",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:57:58.234950Z",
          "iopub.execute_input": "2024-05-20T12:57:58.235540Z",
          "iopub.status.idle": "2024-05-20T12:57:58.472022Z",
          "shell.execute_reply.started": "2024-05-20T12:57:58.235491Z",
          "shell.execute_reply": "2024-05-20T12:57:58.470529Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparing the training images and captions\n",
        "train_imgs = []\n",
        "train_captions = []\n",
        "\n",
        "for imgt in img_name_train_keys:\n",
        "    capt_len = len(img_to_cap_vector[imgt])           # Get the number of captions for the current image\n",
        "    train_imgs.extend([imgt] * capt_len)              # Add the image key multiple times, once for each caption\n",
        "    train_captions.extend(img_to_cap_vector[imgt])    # Add all captions for the current image\n",
        "\n",
        "\n",
        "# Preparing the validation images and captions similarly\n",
        "val_imgs = []\n",
        "val_captions = []\n",
        "\n",
        "for imgv in img_name_val_keys:\n",
        "    capv_len = len(img_to_cap_vector[imgv])         # Get the number of captions for the current image\n",
        "    val_imgs.extend([imgv] * capv_len)              # Add the image key multiple times, once for each caption\n",
        "    val_captions.extend(img_to_cap_vector[imgv])    # Add all captions for the current image"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-05-20T12:58:05.547546Z",
          "iopub.execute_input": "2024-05-20T12:58:05.549120Z",
          "iopub.status.idle": "2024-05-20T12:58:05.687710Z",
          "shell.execute_reply.started": "2024-05-20T12:58:05.549059Z",
          "shell.execute_reply": "2024-05-20T12:58:05.686416Z"
        },
        "trusted": true,
        "id": "6pdA4fFM395W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_imgs), len(train_captions), len(val_imgs), len(val_captions)"
      ],
      "metadata": {
        "id": "UHN3Q1YDN5TD",
        "execution": {
          "iopub.status.busy": "2024-05-20T12:58:19.367532Z",
          "iopub.execute_input": "2024-05-20T12:58:19.368025Z",
          "iopub.status.idle": "2024-05-20T12:58:19.377488Z",
          "shell.execute_reply.started": "2024-05-20T12:58:19.367988Z",
          "shell.execute_reply": "2024-05-20T12:58:19.375956Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(img_path, caption):\n",
        "    img = tf.io.read_file(img_path)                                   # Read the image file from the given path\n",
        "    img = tf.io.decode_jpeg(img, channels=3)                          # Decode the image as a JPEG file, resulting in a 3D tensor\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)                     # Resize the image to the size expected by Inception V3 model\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)    # Preprocess the image using Inception V3's preprocessing function\n",
        "    caption = tokenizer(caption)                                      # Tokenize the caption\n",
        "    return img, caption"
      ],
      "metadata": {
        "id": "12c-7FHzOFSq",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:08:17.408322Z",
          "iopub.execute_input": "2024-05-20T13:08:17.408956Z",
          "iopub.status.idle": "2024-05-20T13:08:17.418574Z",
          "shell.execute_reply.started": "2024-05-20T13:08:17.408900Z",
          "shell.execute_reply": "2024-05-20T13:08:17.416723Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (train_imgs, train_captions))\n",
        "\n",
        "train_dataset = train_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (val_imgs, val_captions))\n",
        "\n",
        "val_dataset = val_dataset.map(\n",
        "    load_data, num_parallel_calls=tf.data.AUTOTUNE\n",
        "    ).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "vHk83y3eOFPz",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:08:23.658284Z",
          "iopub.execute_input": "2024-05-20T13:08:23.658806Z",
          "iopub.status.idle": "2024-05-20T13:08:24.734031Z",
          "shell.execute_reply.started": "2024-05-20T13:08:23.658764Z",
          "shell.execute_reply": "2024-05-20T13:08:24.732889Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Augmentation Model**"
      ],
      "metadata": {
        "id": "gtPG3wXO395Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_augmentation = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.RandomFlip(\"horizontal\"),    # Randomly flip images horizontally\n",
        "        tf.keras.layers.RandomRotation(0.2),         # Randomly rotate images by up to ±40 degrees\n",
        "        tf.keras.layers.RandomContrast(0.3),         # Randomly adjust the contrast of images by up to ±30%\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "bQr_bgk11eMF",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:08:43.832496Z",
          "iopub.execute_input": "2024-05-20T13:08:43.833065Z",
          "iopub.status.idle": "2024-05-20T13:08:43.856541Z",
          "shell.execute_reply.started": "2024-05-20T13:08:43.833019Z",
          "shell.execute_reply": "2024-05-20T13:08:43.854427Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder**"
      ],
      "metadata": {
        "id": "dbQrdJ6l395b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_Encoder():\n",
        "    # Initializing the InceptionV3 model with pretrained ImageNet weights,\n",
        "    # excluding the top classification layer\n",
        "    inception_v3 = tf.keras.applications.InceptionV3(\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Get the output of the last layer of the InceptionV3 model\n",
        "    output = inception_v3.output\n",
        "    # Reshape the output to a 2D tensor to be used as input for the RNN decoder\n",
        "    output = tf.keras.layers.Reshape(\n",
        "        (-1, output.shape[-1]))(output)\n",
        "\n",
        "    # Create a new model that takes an image as input and outputs the reshaped tensor\n",
        "    cnn_model = tf.keras.models.Model(inception_v3.input, output)\n",
        "    # Return the CNN encoder model\n",
        "    return cnn_model"
      ],
      "metadata": {
        "id": "H9GDJ9_1nIMO",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:13:20.702580Z",
          "iopub.execute_input": "2024-05-20T13:13:20.703105Z",
          "iopub.status.idle": "2024-05-20T13:13:20.712236Z",
          "shell.execute_reply.started": "2024-05-20T13:13:20.703068Z",
          "shell.execute_reply": "2024-05-20T13:13:20.710897Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        # Layer normalization layers\n",
        "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
        "        # Multi-head attention layer\n",
        "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        # Dense layer with ReLU activation\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim, activation=\"relu\")\n",
        "\n",
        "    def call(self, x, training):\n",
        "        # Apply the first layer normalization\n",
        "        x = self.layer_norm_1(x)\n",
        "        # Apply the dense layer\n",
        "        x = self.dense(x)\n",
        "        # Compute the attention output\n",
        "        attn_output = self.attention(\n",
        "            query=x,\n",
        "            value=x,\n",
        "            key=x,\n",
        "            attention_mask=None,\n",
        "            training=training\n",
        "        )\n",
        "        # Apply the second layer normalization with residual connection\n",
        "        x = self.layer_norm_2(x + attn_output)\n",
        "        return x"
      ],
      "metadata": {
        "id": "jMy5MrE2PdHV",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:16:36.050621Z",
          "iopub.execute_input": "2024-05-20T13:16:36.051146Z",
          "iopub.status.idle": "2024-05-20T13:16:36.062907Z",
          "shell.execute_reply.started": "2024-05-20T13:16:36.051110Z",
          "shell.execute_reply": "2024-05-20T13:16:36.061442Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Embeddings(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embed_dim, max_len):\n",
        "        super().__init__()\n",
        "        # Token embedding layer\n",
        "        self.token_embeddings = tf.keras.layers.Embedding(\n",
        "            vocab_size, embed_dim)\n",
        "        # Position embedding layer\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            max_len, embed_dim, input_shape=(None, max_len))\n",
        "\n",
        "    def call(self, input_ids):\n",
        "        # Determine the length of the input sequence\n",
        "        length = tf.shape(input_ids)[-1]\n",
        "        # Create a range of position IDs\n",
        "        position_ids = tf.range(start=0, limit=length, delta=1)\n",
        "        # Add a new dimension to make position_ids a 2D tensor\n",
        "        position_ids = tf.expand_dims(position_ids, axis=0)\n",
        "\n",
        "        # Get the token embeddings for the input_ids\n",
        "        token_embeddings = self.token_embeddings(input_ids)\n",
        "        # Get the position embeddings for the position_ids\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        # Return the sum of the token and position embeddings\n",
        "        return token_embeddings + position_embeddings\n"
      ],
      "metadata": {
        "id": "MFqNFts0duGB",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:18:35.949340Z",
          "iopub.execute_input": "2024-05-20T13:18:35.949767Z",
          "iopub.status.idle": "2024-05-20T13:18:35.959982Z",
          "shell.execute_reply.started": "2024-05-20T13:18:35.949733Z",
          "shell.execute_reply": "2024-05-20T13:18:35.958985Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Transformer Decoder Layer class\n",
        "class TransformerDecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    # Initialize the layer with embedding dimension, units, and number of heads\n",
        "    def __init__(self, embed_dim, units, num_heads):\n",
        "        super().__init__()\n",
        "        # Embedding layer for token and position embeddings\n",
        "        self.embedding = Embeddings(\n",
        "            tokenizer.vocabulary_size(), embed_dim, MAX_LENGTH)\n",
        "\n",
        "        # First multi-head attention layer with dropout\n",
        "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "        # Second multi-head attention layer for encoder-decoder attention with dropout\n",
        "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim, dropout=0.1\n",
        "        )\n",
        "\n",
        "        # Layer normalization layers\n",
        "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
        "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "        # Feed-forward network layers\n",
        "        self.ffn_layer_1 = tf.keras.layers.Dense(units, activation=\"relu\")\n",
        "        self.ffn_layer_2 = tf.keras.layers.Dense(embed_dim)\n",
        "\n",
        "        # Output layer with softmax activation for generating predictions\n",
        "        self.out = tf.keras.layers.Dense(tokenizer.vocabulary_size(), activation=\"softmax\")\n",
        "\n",
        "        # Dropout layers for regularization\n",
        "        self.dropout_1 = tf.keras.layers.Dropout(0.3)\n",
        "        self.dropout_2 = tf.keras.layers.Dropout(0.5)\n",
        "\n",
        "    # The call method for the layer, defining its forward pass\n",
        "    def call(self, input_ids, encoder_output, training, mask=None):\n",
        "        # Obtain embeddings for the input IDs\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        # Initialize masks for attention\n",
        "        combined_mask = None\n",
        "        padding_mask = None\n",
        "\n",
        "        # Create masks if provided\n",
        "        if mask is not None:\n",
        "            # Get causal attention mask to prevent future tokens from being attended to\n",
        "            causal_mask = self.get_causal_attention_mask(embeddings)\n",
        "            # Create padding mask for attention\n",
        "            padding_mask = tf.cast(mask[:, :, tf.newaxis], dtype=tf.int32)\n",
        "            # Combine masks\n",
        "            combined_mask = tf.cast(mask[:, tf.newaxis, :], dtype=tf.int32)\n",
        "            combined_mask = tf.minimum(combined_mask, causal_mask)\n",
        "\n",
        "        # Apply the first attention layer with the combined mask\n",
        "        attn_output_1 = self.attention_1(\n",
        "            query=embeddings,\n",
        "            value=embeddings,\n",
        "            key=embeddings,\n",
        "            attention_mask=combined_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        # Apply layer normalization after adding the attention output to embeddings\n",
        "        out_1 = self.layernorm_1(embeddings + attn_output_1)\n",
        "\n",
        "        # Apply the second attention layer with the padding mask\n",
        "        attn_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_output,\n",
        "            key=encoder_output,\n",
        "            attention_mask=padding_mask,\n",
        "            training=training\n",
        "        )\n",
        "\n",
        "        # Apply layer normalization after adding the second attention output\n",
        "        out_2 = self.layernorm_2(out_1 + attn_output_2)\n",
        "\n",
        "        # Apply the feed-forward network and dropout\n",
        "        ffn_out = self.ffn_layer_1(out_2)\n",
        "        ffn_out = self.dropout_1(ffn_out, training=training)\n",
        "        ffn_out = self.ffn_layer_2(ffn_out)\n",
        "\n",
        "        # Apply the final layer normalization and dropout\n",
        "        ffn_out = self.layernorm_3(ffn_out + out_2)\n",
        "        ffn_out = self.dropout_2(ffn_out, training=training)\n",
        "        # Generate predictions using the output layer\n",
        "        preds = self.out(ffn_out)\n",
        "        return preds\n",
        "\n",
        "    # Helper function to create a causal attention mask\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        # Get the shape of the inputs\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        # Create a matrix where the upper triangle is zeroed out\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        # Repeat the mask for each element in the batch\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n"
      ],
      "metadata": {
        "id": "pcbCQqrDnJ4-",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:28:56.402278Z",
          "iopub.execute_input": "2024-05-20T13:28:56.402856Z",
          "iopub.status.idle": "2024-05-20T13:28:56.433333Z",
          "shell.execute_reply.started": "2024-05-20T13:28:56.402802Z",
          "shell.execute_reply": "2024-05-20T13:28:56.431805Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom Keras Model for Image Captioning\n",
        "class ImageCaptioningModel(tf.keras.Model):\n",
        "\n",
        "    # Initialize the model with CNN encoder, Transformer encoder and decoder, and optional image augmentation\n",
        "    def __init__(self, cnn_model, encoder, decoder, image_aug=None):\n",
        "        super().__init__()\n",
        "        self.cnn_model = cnn_model  # CNN model for feature extraction from images\n",
        "        self.encoder = encoder  # Transformer encoder\n",
        "        self.decoder = decoder  # Transformer decoder\n",
        "        self.image_aug = image_aug  # Optional image augmentation\n",
        "        self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")  # Tracker for loss\n",
        "        self.acc_tracker = tf.keras.metrics.Mean(name=\"accuracy\")  # Tracker for accuracy\n",
        "\n",
        "    # Calculate the loss, taking into account the mask for padded tokens\n",
        "    def calculate_loss(self, y_true, y_pred, mask):\n",
        "        loss = self.loss(y_true, y_pred)  # Compute the loss using the model's loss function\n",
        "        mask = tf.cast(mask, dtype=loss.dtype)  # Cast the mask to the same dtype as the loss\n",
        "        loss *= mask  # Apply the mask to the loss\n",
        "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)  # Return the average loss\n",
        "\n",
        "    # Calculate the accuracy, taking into account the mask for padded tokens\n",
        "    def calculate_accuracy(self, y_true, y_pred, mask):\n",
        "        accuracy = tf.equal(y_true, tf.argmax(y_pred, axis=2))  # Check if predictions match the true values\n",
        "        accuracy = tf.math.logical_and(mask, accuracy)  # Apply the mask to the accuracy\n",
        "        accuracy = tf.cast(accuracy, dtype=tf.float32)  # Cast the accuracy to float32\n",
        "        mask = tf.cast(mask, dtype=tf.float32)  # Cast the mask to float32\n",
        "        return tf.reduce_sum(accuracy) / tf.reduce_sum(mask)  # Return the average accuracy\n",
        "\n",
        "    # Compute the loss and accuracy for a batch of data\n",
        "    def compute_loss_and_acc(self, img_embed, captions, training=True):\n",
        "        encoder_output = self.encoder(img_embed, training=training)  # Get encoder output\n",
        "        y_input = captions[:, :-1]  # Input for the decoder (exclude the last token)\n",
        "        y_true = captions[:, 1:]  # True output for the decoder (exclude the first token)\n",
        "        mask = (y_true != 0)  # Create a mask for non-zero tokens\n",
        "        y_pred = self.decoder(\n",
        "            y_input, encoder_output, training=training, mask=mask\n",
        "        )  # Get decoder predictions\n",
        "        loss = self.calculate_loss(y_true, y_pred, mask)  # Calculate loss\n",
        "        acc = self.calculate_accuracy(y_true, y_pred, mask)  # Calculate accuracy\n",
        "        return loss, acc\n",
        "\n",
        "    # Perform a training step\n",
        "    def train_step(self, batch):\n",
        "        imgs, captions = batch  # Unpack the batch\n",
        "\n",
        "        if self.image_aug:  # If image augmentation is provided\n",
        "            imgs = self.image_aug(imgs)  # Apply image augmentation\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)  # Get image embeddings from the CNN model\n",
        "\n",
        "        with tf.GradientTape() as tape:  # Record operations for automatic differentiation\n",
        "            loss, acc = self.compute_loss_and_acc(\n",
        "                img_embed, captions\n",
        "            )  # Compute loss and accuracy\n",
        "\n",
        "        # Get trainable variables from the encoder and decoder\n",
        "        train_vars = (\n",
        "            self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "        )\n",
        "        grads = tape.gradient(loss, train_vars)  # Compute gradients\n",
        "        self.optimizer.apply_gradients(zip(grads, train_vars))  # Apply gradients to variables\n",
        "        self.loss_tracker.update_state(loss)  # Update the loss tracker\n",
        "        self.acc_tracker.update_state(acc)  # Update the accuracy tracker\n",
        "\n",
        "        # Return the current loss and accuracy\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    # Perform a validation step\n",
        "    def test_step(self, batch):\n",
        "        imgs, captions = batch  # Unpack the batch\n",
        "\n",
        "        img_embed = self.cnn_model(imgs)  # Get image embeddings from the CNN model\n",
        "\n",
        "        loss, acc = self.compute_loss_and_acc(\n",
        "            img_embed, captions, training=False  # Compute loss and accuracy in inference mode\n",
        "        )\n",
        "\n",
        "        self.loss_tracker.update_state(loss)  # Update the loss tracker\n",
        "        self.acc_tracker.update_state(acc)  # Update the accuracy tracker\n",
        "\n",
        "        # Return the current loss and accuracy\n",
        "        return {\"loss\": self.loss_tracker.result(), \"acc\": self.acc_tracker.result()}\n",
        "\n",
        "    # Define which metrics should be tracked\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_tracker, self.acc_tracker]\n"
      ],
      "metadata": {
        "id": "9_NmSUaVys9R",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:34:10.442759Z",
          "iopub.execute_input": "2024-05-20T13:34:10.443379Z",
          "iopub.status.idle": "2024-05-20T13:34:10.472367Z",
          "shell.execute_reply.started": "2024-05-20T13:34:10.443331Z",
          "shell.execute_reply": "2024-05-20T13:34:10.471162Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = TransformerEncoderLayer(EMBEDDING_DIM, 1)\n",
        "decoder = TransformerDecoderLayer(EMBEDDING_DIM, UNITS, 8)\n",
        "\n",
        "cnn_model = CNN_Encoder()\n",
        "caption_model = ImageCaptioningModel(\n",
        "    cnn_model=cnn_model, encoder=encoder, decoder=decoder, image_aug=image_augmentation,\n",
        ")"
      ],
      "metadata": {
        "id": "GqWpcsje0Hkh",
        "execution": {
          "iopub.status.busy": "2024-05-20T13:48:12.588830Z",
          "iopub.execute_input": "2024-05-20T13:48:12.589431Z",
          "iopub.status.idle": "2024-05-20T13:48:34.958580Z",
          "shell.execute_reply.started": "2024-05-20T13:48:12.589385Z",
          "shell.execute_reply": "2024-05-20T13:48:34.955136Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction=\"none\"\n",
        ")\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "caption_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=cross_entropy\n",
        ")"
      ],
      "metadata": {
        "id": "bayNssgNX6QN",
        "execution": {
          "iopub.status.busy": "2024-04-21T03:30:05.450431Z",
          "iopub.execute_input": "2024-04-21T03:30:05.450733Z",
          "iopub.status.idle": "2024-04-21T03:30:05.466052Z",
          "shell.execute_reply.started": "2024-04-21T03:30:05.450706Z",
          "shell.execute_reply": "2024-04-21T03:30:05.46518Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "1RYo-MRVYn49",
        "execution": {
          "iopub.status.busy": "2024-04-21T03:30:05.467285Z",
          "iopub.execute_input": "2024-04-21T03:30:05.467746Z",
          "iopub.status.idle": "2024-04-21T04:39:40.582629Z",
          "shell.execute_reply.started": "2024-04-21T03:30:05.467711Z",
          "shell.execute_reply": "2024-04-21T04:39:40.581634Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:18:07.077071Z",
          "iopub.execute_input": "2024-04-21T05:18:07.077516Z",
          "iopub.status.idle": "2024-04-21T05:44:00.131224Z",
          "shell.execute_reply.started": "2024-04-21T05:18:07.077481Z",
          "shell.execute_reply": "2024-04-21T05:44:00.130366Z"
        },
        "trusted": true,
        "id": "vT6F4Nhs395h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "iBvle_Fa395h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = caption_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:55:23.778938Z",
          "iopub.execute_input": "2024-04-21T05:55:23.779698Z",
          "iopub.status.idle": "2024-04-21T05:55:39.039515Z",
          "shell.execute_reply.started": "2024-04-21T05:55:23.779661Z",
          "shell.execute_reply": "2024-04-21T05:55:39.038202Z"
        },
        "trusted": true,
        "id": "36s_tjGT395i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:55:42.899045Z",
          "iopub.execute_input": "2024-04-21T05:55:42.899769Z",
          "iopub.status.idle": "2024-04-21T05:55:43.304944Z",
          "shell.execute_reply.started": "2024-04-21T05:55:42.899735Z",
          "shell.execute_reply": "2024-04-21T05:55:43.30221Z"
        },
        "trusted": true,
        "id": "0TKWnso0395i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image_from_path(img_path):\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.keras.layers.Resizing(299, 299)(img)\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def generate_caption(img_path, add_noise=False):\n",
        "    img = load_image_from_path(img_path)\n",
        "\n",
        "    if add_noise:\n",
        "        noise = tf.random.normal(img.shape)*0.1\n",
        "        img = img + noise\n",
        "        img = (img - tf.reduce_min(img))/(tf.reduce_max(img) - tf.reduce_min(img))\n",
        "\n",
        "    img = tf.expand_dims(img, axis=0)\n",
        "    img_embed = caption_model.cnn_model(img)\n",
        "    img_encoded = caption_model.encoder(img_embed, training=False)\n",
        "\n",
        "    y_inp = '[start]'\n",
        "    for i in range(MAX_LENGTH-1):\n",
        "        tokenized = tokenizer([y_inp])[:, :-1]\n",
        "        mask = tf.cast(tokenized != 0, tf.int32)\n",
        "        pred = caption_model.decoder(\n",
        "            tokenized, img_encoded, training=False, mask=mask)\n",
        "\n",
        "        pred_idx = np.argmax(pred[0, i, :])\n",
        "        pred_idx = tf.convert_to_tensor(pred_idx)\n",
        "        pred_word = idx2word(pred_idx).numpy().decode('utf-8')\n",
        "        if pred_word == '[end]':\n",
        "            break\n",
        "\n",
        "        y_inp += ' ' + pred_word\n",
        "\n",
        "    y_inp = y_inp.replace('[start] ', '')\n",
        "    return y_inp"
      ],
      "metadata": {
        "id": "3ErlQQICtj_g",
        "execution": {
          "iopub.status.busy": "2024-04-21T05:55:46.889193Z",
          "iopub.execute_input": "2024-04-21T05:55:46.889562Z",
          "iopub.status.idle": "2024-04-21T05:55:46.902742Z",
          "shell.execute_reply.started": "2024-04-21T05:55:46.889533Z",
          "shell.execute_reply": "2024-04-21T05:55:46.901577Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "id": "27_bJe_M1Drr",
        "execution": {
          "iopub.status.busy": "2024-04-21T05:55:50.488647Z",
          "iopub.execute_input": "2024-04-21T05:55:50.489Z",
          "iopub.status.idle": "2024-04-21T05:55:51.02225Z",
          "shell.execute_reply.started": "2024-04-21T05:55:50.488972Z",
          "shell.execute_reply": "2024-04-21T05:55:51.021332Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img_url = \"https://images.squarespace-cdn.com/content/v1/5e0e65adcd39ed279a0402fd/1627422658456-7QKPXTNQ34W2OMBTESCJ/1.jpg?format=2500w\"\n",
        "\n",
        "# im = Image.open(requests.get(img_url, stream=True).raw)\n",
        "# im = im.convert('RGB')\n",
        "# im.save('tmp.jpg')\n",
        "\n",
        "# pred_caption = generate_caption('tmp.jpg', add_noise=False)\n",
        "# print('Predicted Caption:', pred_caption)\n",
        "# print()\n",
        "# im"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:07:10.216478Z",
          "iopub.execute_input": "2024-04-21T05:07:10.217176Z",
          "iopub.status.idle": "2024-04-21T05:07:10.222011Z",
          "shell.execute_reply.started": "2024-04-21T05:07:10.217138Z",
          "shell.execute_reply": "2024-04-21T05:07:10.220964Z"
        },
        "trusted": true,
        "id": "sqASrDOL395k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_url = \"https://cdn.pixabay.com/photo/2016/10/26/22/02/dog-1772759_1280.jpg\"\n",
        "\n",
        "im = Image.open(requests.get(img_url, stream=True).raw)\n",
        "im = im.convert('RGB')\n",
        "im.save('tmp.jpg')\n",
        "\n",
        "pred_caption = generate_caption('tmp.jpg', add_noise=False)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "im"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:55:57.92849Z",
          "iopub.execute_input": "2024-04-21T05:55:57.929415Z",
          "iopub.status.idle": "2024-04-21T05:55:58.884009Z",
          "shell.execute_reply.started": "2024-04-21T05:55:57.929373Z",
          "shell.execute_reply": "2024-04-21T05:55:58.882979Z"
        },
        "trusted": true,
        "id": "GHl0cF0f395l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caption_model.save_weights('model.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:07:11.445574Z",
          "iopub.execute_input": "2024-04-21T05:07:11.445912Z",
          "iopub.status.idle": "2024-04-21T05:07:11.938166Z",
          "shell.execute_reply.started": "2024-04-21T05:07:11.445881Z",
          "shell.execute_reply": "2024-04-21T05:07:11.937265Z"
        },
        "trusted": true,
        "id": "KNwdJKXq395m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "id": "XG69m29gs6W4",
        "execution": {
          "iopub.status.busy": "2024-04-21T05:58:06.208727Z",
          "iopub.execute_input": "2024-04-21T05:58:06.209086Z",
          "iopub.status.idle": "2024-04-21T05:58:06.801485Z",
          "shell.execute_reply.started": "2024-04-21T05:58:06.209056Z",
          "shell.execute_reply": "2024-04-21T05:58:06.800389Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:58:03.761134Z",
          "iopub.execute_input": "2024-04-21T05:58:03.761983Z",
          "iopub.status.idle": "2024-04-21T05:58:04.434308Z",
          "shell.execute_reply.started": "2024-04-21T05:58:03.761941Z",
          "shell.execute_reply": "2024-04-21T05:58:04.433294Z"
        },
        "trusted": true,
        "id": "JeUTaKGo396B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:58:24.298497Z",
          "iopub.execute_input": "2024-04-21T05:58:24.298888Z",
          "iopub.status.idle": "2024-04-21T05:58:25.040429Z",
          "shell.execute_reply.started": "2024-04-21T05:58:24.298856Z",
          "shell.execute_reply": "2024-04-21T05:58:25.03951Z"
        },
        "trusted": true,
        "id": "4e34UFGF396B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:58:32.07872Z",
          "iopub.execute_input": "2024-04-21T05:58:32.079101Z",
          "iopub.status.idle": "2024-04-21T05:58:32.690179Z",
          "shell.execute_reply.started": "2024-04-21T05:58:32.079062Z",
          "shell.execute_reply": "2024-04-21T05:58:32.689278Z"
        },
        "trusted": true,
        "id": "NFj3MVKU396C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = random.randrange(0, len(captions))\n",
        "img_path = captions.iloc[idx].image\n",
        "\n",
        "pred_caption = generate_caption(img_path)\n",
        "print('Predicted Caption:', pred_caption)\n",
        "print()\n",
        "Image.open(img_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T05:58:41.198802Z",
          "iopub.execute_input": "2024-04-21T05:58:41.199196Z",
          "iopub.status.idle": "2024-04-21T05:58:41.858864Z",
          "shell.execute_reply.started": "2024-04-21T05:58:41.199162Z",
          "shell.execute_reply": "2024-04-21T05:58:41.857936Z"
        },
        "trusted": true,
        "id": "bJrDgMrE396C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to generate and display 10 predictions with corresponding images\n",
        "def display_predictions(num_predictions=10):\n",
        "    # Set up the subplot grid\n",
        "    num_cols = 2\n",
        "    num_rows = (num_predictions + num_cols - 1) // num_cols\n",
        "    plt.figure(figsize=(15, 5 * num_rows))\n",
        "\n",
        "    for i in range(num_predictions):\n",
        "        # Generate a random index and corresponding image path\n",
        "        idx = random.randrange(0, len(captions))\n",
        "        img_path = captions.iloc[idx].image\n",
        "\n",
        "        # Generate a caption for the image\n",
        "        pred_caption = generate_caption(img_path)\n",
        "\n",
        "        # Plot the image with the predicted caption\n",
        "        plt.subplot(num_rows, num_cols, i + 1)\n",
        "        plt.imshow(Image.open(img_path))\n",
        "        plt.title(f'Prediction {i+1}: {pred_caption}')\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function to display the predictions\n",
        "display_predictions(10)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-21T06:02:20.221822Z",
          "iopub.execute_input": "2024-04-21T06:02:20.222709Z",
          "iopub.status.idle": "2024-04-21T06:02:27.00483Z",
          "shell.execute_reply.started": "2024-04-21T06:02:20.22267Z",
          "shell.execute_reply": "2024-04-21T06:02:27.00335Z"
        },
        "trusted": true,
        "id": "_lZs6yjV396D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "jxeOE_ma396D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yJMJgkF396E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}